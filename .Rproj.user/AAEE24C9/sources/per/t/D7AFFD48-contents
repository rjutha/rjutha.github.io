---
title: "How to enhance A/B testing with the Multi-arm Bandit"
description: "An alternative algorithm for testing that leverages updating sampling frequencies for a more rapid decision than traditional apporaches."
bibliography: resource.bib
twitter-card:
  image: "featured.png"
author:
  - name: Rahim Jutha
    url: https://rjutha.github.io/
date: 10-04-2023
categories: [Data-Viz, R, TidyTuesday, Shiny, Bayesian] # self-defined categories
citation:
  url: https://rjutha.github.io/posts/posts/Multi-Arm-Bandit-Algorithm/
image: "featured.png"
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready
---

Hello There!

Today I'm going to be discussing an algorithm called the **Multi-Arm Bandit**, which is used in the realm of statistical testing. I came across the algorithm when reading the book *"Practical Statistics for Data Scientists"*. The book introduced the algorithm as "an approach to testing ... that allows explicit optimization and more rapid decision making than the traditional statistical approach to designing experiments" [@psfds]. Not only does the algorithm have a fun name, it also has a clear advantage when trying to optimize for the business rather than statistical significance.

When compared to a traditional **A/B test**, the multi-arm bandit leverages taking advantage of results prior to the completion of an experiment. By utilizing the computing power of modern day tools, sampling can be updated efficiently during the sampling process to reach conclusions faster than traditional methods.

The algorithm is a **reinforced learning** which allows it to update itself using trial and error. An easy comparison to make is how in **Bayesian Statistics**, updating a prior distribution using evidence to obtain a new posterior distribution is a common idea and is essentially what this algorithm does.

The Multi-Arm Bandit algorithm is widely used web design and marketing. This is because the results in these fields are easily measured. Since the core advantage of this algorithm involves being able to alter the sampling process during the experiment, these fields work together nicely with the algorithm.

For the remainder of this post, I'm going to compare the methods and provide examples to see how they work.

## Set-up for examples

In order to showcase the different methods I'm going to simulate an example business problem to show the differences between A/B testing and the Multi-Arm Bandit algorithm. Note that my example may not be the most real-life scenario but the goal of this is to show how to implement these methods.

Let's say we're a company wanting to run an experiment to find out which of their 2 advertisements performs best. We decide to run the experiment on the next 3000 customers by showing them a different advertisement and using the results to decide which one to deploy.

Unbeknownst to us, Advertisement A has a 10% conversion rate and Advertisement B has a 15% conversion rate. These values will allow us to generate a sample for us to make inference on.

### Load Libraries

```{r}
#| warning: FALSE
#| output: FALSE
library(tidyverse)
library(kableExtra)
library(ggtext)
library(glue)
library(scales)
```

## A/B testing

A/B tests have been around since the early 1900s and is still a commonly used technique today. Businesses around the world use A/B testing to make data-driven decisions.

Since I have already decided to use the next 3000 samples for testing. I've decided to simply equally split them between the two advertisements. Below I simulate the samples for both groups, calculate the sample conversion rate, and create a contingency table to show the results.

```{r}
set.seed(423)

A_sample <- sample(0:1, 1500, replace = TRUE , prob = c(0.9,0.1))
B_sample <- sample(0:1, 1500, replace = TRUE , prob = c(0.85,0.15))

contingency_table <-
  tibble(
    Outcome = c('Conversion', 'No Conversion'),
    `Advertisement A` = c(sum(A_sample), 1500 - sum(A_sample)),
    `Advertisement B` = c(sum(B_sample), 1500 - sum(B_sample))
  ) %>%
  column_to_rownames("Outcome") %>%
  as.matrix()

A_conversion_rate <- contingency_table[1,1] / sum(contingency_table[,1])
A_conversion_rate_95_moe <- 1.96 * sd(A_sample) / sqrt(1500)

B_conversion_rate <- contingency_table[1,2] / sum(contingency_table[,2])
B_conversion_rate_95_moe <- 1.96 * sd(B_sample) / sqrt(1500)

kable(contingency_table) %>%
  kable_styling('basic')
```

In A/B testing there is a decision on what test to use to test for significance. Common options are: z-tests, t=tests, chi-squared tests, fisher's exact test. Since I'm dealing with counts in this example I decided to use a chi-squared test. The test shows that there is a statisically diffence between the conversion rate of the two advertisements and that the advertisement B performs better.

```{r}
result <- chisq.test(contingency_table)
result
```

Below is a quick plot that summarized the results.

```{r, fig.width= 6, fig.height=4}
#| code-fold: TRUE
colors = c('grey92', '#0ca823')
  
title = glue('A/B Test show Advertisement B <span style = "color:{colors[2]}">**Performs Better**</span>.')

subtitle = glue('Chi-squared test with a p-value of {format.pval(result$p.value, digits = 2)} shows their <span style = "color:{colors[2]}">**Converstion Rates**</span> are<br>significantly different. A sample size of 1500 customers were used for each ad.')


contingency_table %>%
  as_tibble(rownames = 'Outcome') %>%
  pivot_longer(cols = 2:3, names_to = "product") %>%
  mutate(
    Outcome = factor(Outcome, levels = c('No Conversion', 'Conversion')),
  )  %>%
  ggplot() +
  geom_col(aes(x = value, y = product, fill = Outcome),
           show.legend = FALSE) +
  theme_minimal() +
  coord_flip(clip = "off") +
  scale_fill_manual(
    values = colors
  ) +
  labs(
    title = title,
    subtitle = subtitle,
    x = NULL,
    y = NULL
    ) +
  theme(
    plot.title.position = 'plot',
    plot.title = element_markdown(size = 18),
    plot.subtitle = element_markdown(size = 12, lineheight = 1.2, margin = margin(0,0,0.5,0, unit = 'cm')),
    axis.text.x  = element_blank(),
    axis.line.x  = element_blank(),
    axis.text.y  = element_blank(),
    axis.line.y  = element_blank(),
    panel.grid = element_blank()
  ) +
  annotate(
    'text',
    x = 1700,
    y = 1,
    size = 5,
    label = 'Advertisement A'
  ) +
    annotate(
    'text',
    x = 1700,
    y = 2,
    size = 5,
    label = 'Advertisement B'
    ) +
  annotate(
    'text',
    x = 750,
    y = 1,
    size = 6,
    label = glue('{percent(A_conversion_rate, accuracy = 0.01)} +/- {percent(A_conversion_rate_95_moe, scale = 100, accuracy = 0.01)}'),
    color = colors[2],
    fontface = 'bold'
  ) +
  annotate(
    'text',
    x = 750,
    y = 2,
    size = 6,
    label = glue('{percent(B_conversion_rate, accuracy = 0.01)} +/- {percent(B_conversion_rate_95_moe, scale = 100, accuracy = 0.01)}'),
    color = colors[2],
    fontface = 'bold'
  )
```

## Multi-Arm-Bandit

To explain how the multi-arm bandit works I present an example. Imagine you had 4 slot machines in front of you, all with different probability distributions. Instead of playing them all an equally amount of times we want to try to optimize the amount we play the one with the highest likelihood of winning. To do this we play the slot machines and as we play them we them all a certain number of times and decide on a best one.

If it's truly the best one, then great. But if its not we lose the opportunity to find the best one. What the multi-arm bandit algorithm does in this model is we start pulling our best one more often but we don't abandon pulling the other ones. As this "best" one outperforms or under performs we shift resources away and towards this group. By the end of the experiment the superior group will have received the most samples which is beneficial when you're trying to make business decisions.

The book *"Practical Statistics for Data Scientists"* describes a simple implementation called the epsilon-greedy algorithm for an A/B test.

1.  Generate random number between 0 and 1.

2.  If the number lies between 0 and epsilon (where epsilon is a number between 0 and 1, typically fairly small), flip a fair coin (50/50 probability), and:

    If the coin is heads, show  A.

    If the coin is tails, show  B.

3. If the number is â‰¥ epsilon, show whichever has peformed best to date.

I was planning to create my own implementation fo this but I found an R package called contextual that has an implementation of it. An R package facilitating the simulation and evaluation of context-free and contextual Multi-Armed Bandit policies.

https://www.rdocumentation.org/packages/contextual/versions/0.9.8.4


```{r}

```
  
## Thompson's sampling uses a Bayesian approach:

same concept using a beta prior instead seems better will read about it and provide a breif example

https://www.r-bloggers.com/2019/09/multi-armed-bandits-as-an-a-b-testing-solution/
